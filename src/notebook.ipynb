{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🍫 Building a Datacomp filtering pipeline with Fondant\n",
    "\n",
    "[DataComp](https://www.datacomp.ai/) is a competition organized by the University of Washington and\n",
    "others to come up with the best possible image-text dataset to train a fixed CLIP model. Hence, it's\n",
    "an ideal use case for Fondant, as we can leverage reusable components to filter large, noisy\n",
    "image-text datasets.\n",
    "\n",
    "In this example, we build a pipeline for filtering the dataset using the T-Mars data filtering\n",
    "approach. For more information on T-Mars, check out\n",
    "the [official paper](https://arxiv.org/pdf/2307.03132.pdf). \n",
    "\n",
    "There are 7 components in total, these are:\n",
    "\n",
    "1. [**Load from hf hub**](components/generate_prompts): The pipeline begins by loading the initial\n",
    "   datacomp data which we hosted on the Hugginface hub.\n",
    "\n",
    "2. [**Download images**](https://github.com/ml6team/fondant/tree/main/components/download_images):\n",
    "   This component downloads the actual images based on the URLs retrieved by the previous component.\n",
    "   It takes in the URLs as input and returns the actual images.\n",
    "\n",
    "3. [**Resize images**](https://github.com/ml6team/fondant/tree/main/components/resize_images): This\n",
    "   component resizes the images to a fixed size. It takes in the images as input and returns the\n",
    "   resized images.\n",
    "\n",
    "4. [**Detect text**](components/detect_text): This component detects text in the images using\n",
    "   ann [mmocr model](https://github.com/locuslab/T-MARS/tree/main/dataset2metadata/text_detection).\n",
    "   It takes in the images as input and returns the bounding boxes of the detected text.\n",
    "\n",
    "5. [**Mask images**](components/mask_images): This component masks the detected text in the images.\n",
    "   It takes in the images and the bounding boxes as input and returns the masked images.\n",
    "\n",
    "6. [**Add clip score**](components/add_clip_score): This component adds a CLIP score to the images.\n",
    "   The clip score is estimated as the dot product between the CLIP embeddings of the masked images\n",
    "   and the original image captions.\n",
    "\n",
    "7. [**Filter clip score**](components/filter_clip_score): This component filters the images based on\n",
    "   their CLIP score. It takes in the images and the CLIP scores as input and returns the filtered\n",
    "   indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisite:**\n",
    "\n",
    "- Ensure Python version 3.8 to 3.10 is installed on your system.\n",
    "- Install and configure Docker on your system.\n",
    "- Ensure that you have a GPU for running the GPU-based component of the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Setup your environment \n",
    "!pip install \"fondant[docker]==0.8.0\" -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the pipeline\n",
    "\n",
    "First of all, we need to initialize the pipeline, which includes specifying a name for your pipeline, providing a description, and setting a base_path. The base_path is used to store the pipeline artifacts and data generated by the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import fsspec\n",
    "\n",
    "from fondant.pipeline import Pipeline, Resources\n",
    "from fondant.compiler import DockerCompiler\n",
    "from fondant.runner import DockerRunner\n",
    "\n",
    "# Check GPU\n",
    "try:\n",
    "    subprocess.check_output('nvidia-smi')\n",
    "    number_of_accelerators = 1\n",
    "    accelerator_name = \"GPU\"\n",
    "except Exception:\n",
    "    logging.warning(\"We recommend to run this pipeline on a GPU, but none could be found\")\n",
    "    number_of_accelerators = None\n",
    "    accelerator_name = None\n",
    "    \n",
    "# General configs\n",
    "BASE_PATH = \"./fondant-artifacts\"\n",
    "N_ROWS_TO_LOAD = 10  # Set to None to load all rows\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "# Create data directory if it doesn't exist and if it's a local path\n",
    "if fsspec.core.url_to_fs(BASE_PATH)[0].protocol == ('file', 'local'):\n",
    "    Path(BASE_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"datacomp-filtering-pipeline\",\n",
    "    pipeline_description=\"A pipeline for filtering the Datacomp dataset\",\n",
    "    base_path=BASE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we will use the `load_from_hub_op` component to load the initial [dataset](https://huggingface.co/datasets/nielsr/datacomp-small-with-text-embeddings):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_from_hf_hub = pipeline.read(\n",
    "    \"load_from_hf_hub\",\n",
    "    arguments={\n",
    "        \"dataset_name\": \"nielsr/datacomp-small-with-text-embeddings\",\n",
    "        \"n_rows_to_load\": N_ROWS_TO_LOAD,\n",
    "    },\n",
    "    produces={\n",
    "        \"url\": pa.string(),\n",
    "        \"original_width\": pa.int64(),\n",
    "        \"original_height\": pa.int64(),\n",
    "        \"face_bboxes\": pa.list_(pa.list_(pa.float64())),\n",
    "        \"sha256\": pa.string(),\n",
    "        \"text\": pa.string(),\n",
    "        \"uid\": pa.string(),\n",
    "        \"clip_b32_similarity_score\": pa.float32(),\n",
    "        \"clip_l14_similarity_score\": pa.float32(),\n",
    "        \"clip_l14_text_embedding\": pa.list_(pa.float64())\n",
    "    },\n",
    "    cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our pipeline consists of a single component that loads the dataset from HuggingFace Hub. We can proceed to add the other components. To add a new reusable component, use the `apply` method. We have to pass the name of the component we want to use, as well as component arguments.\n",
    "\n",
    "The `consumes` argument defines which columns of the dataset will be passed to component. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = dataset_from_hf_hub.apply(\n",
    "    \"download_images\",\n",
    "    consumes={\n",
    "        \"image_url\": \"url\"\n",
    "    },\n",
    "    arguments={\n",
    "        \"retries\": 2,\n",
    "        \"min_image_size\": 0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can utilize the `apply` method to incorporate custom components. For this, it is necessary to provide the path to the implementation of the custom component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_images = images.apply(\n",
    "     \"components/resize_images\",\n",
    "     arguments={\n",
    "         \"resize_width\": IMAGE_SIZE,\n",
    "         \"resize_height\": IMAGE_SIZE,\n",
    "     }\n",
    " )\n",
    "\n",
    "detected_text = resized_images.apply(\n",
    "    \"components/detect_text\",\n",
    "    arguments={\n",
    "        \"batch_size\": 8,\n",
    "        \"image_size\": IMAGE_SIZE,\n",
    "    },\n",
    "    resources=Resources(accelerator_name=\"GPU\", accelerator_number=1)\n",
    ")\n",
    "\n",
    "mask_images = detected_text.apply(\n",
    "    \"components/mask_images\"\n",
    ")\n",
    "\n",
    "embedded_images = mask_images.apply(\n",
    "    \"embed_images\",\n",
    "    arguments={\n",
    "        \"batch_size\": 8,\n",
    "    },\n",
    "    resources=Resources(accelerator_name=\"GPU\", accelerator_number=1)\n",
    ")\n",
    "\n",
    "images_with_clip_score = embedded_images.apply(\n",
    "    \"components/add_clip_score\",\n",
    "    consumes={\n",
    "        \"text_embedding\": \"clip_l14_text_embedding\"\n",
    "    }\n",
    ")\n",
    "\n",
    "filtered_clip_score_op = images_with_clip_score.apply(\n",
    "    \"components/filter_clip_score\",\n",
    "    arguments={\n",
    "        \"threshold_score\": 0.19,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: writing the dataset to HF hub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the final dataset to HF hub, we will use the `write_to_hf_hub` component. \n",
    "Now, instead of using the apply method, we will use the `write` method. Additionally, we are including a column mapping in the produce argument to select specific columns that we want to write to the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = \"your-huggingface-username\"\n",
    "HF_TOKEN = \"your-huggingface-token\"\n",
    "\n",
    "filtered_clip_score_op.write(\n",
    "    \"write_to_hf_hub\",\n",
    "    arguments={\n",
    "        \"username\": USERNAME,\n",
    "        \"hf_token\": HF_TOKEN,\n",
    "        \"dataset_name\": \"controlnet-interior-design\",\n",
    "    }\n",
    "    produces={\n",
    "\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline\n",
    "\n",
    "The pipeline will generate the prompts, retreive matching images in the laion dataset and download then and finally will generate corresponding captions and segmentations needed before writing the dataset to the HF hub.\n",
    "\n",
    "We can execute our pipeline. Fondant provides various executors, and in this case, we are using the `DockerRunner` for local execution, which utilizes docker-compose under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.pipeline.runner import DockerRunner\n",
    "DockerRunner().run(input=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explore the dataset using the fondant explorer, this enables you to visualize your output dataset at each component step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "run_explorer_app(base_path=BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up\n",
    "\n",
    "If you're happy with your dataset, it's time to scale up. Check [our documentation](https://fondant.ai/en/latest/) for more information about the available runners."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
