{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üç´ Building a Datacomp filtering pipeline with Fondant\n",
    "\n",
    "[DataComp](https://www.datacomp.ai/) is a competition organized by the University of Washington and\n",
    "others to come up with the best possible image-text dataset to train a fixed CLIP model. Hence, it's\n",
    "an ideal use case for Fondant, as we can leverage reusable components to filter large, noisy\n",
    "image-text datasets.\n",
    "\n",
    "In this example, we build a pipeline for filtering the dataset using the T-Mars data filtering\n",
    "approach. For more information on T-Mars, check out\n",
    "the [official paper](https://arxiv.org/pdf/2307.03132.pdf). \n",
    "\n",
    "There are 7 components in total, these are:\n",
    "\n",
    "1. [**Load from hf hub**](components/generate_prompts): The pipeline begins by loading the initial\n",
    "   datacomp data which we hosted on the Hugginface hub.\n",
    "\n",
    "2. [**Download images**](https://github.com/ml6team/fondant/tree/main/components/download_images):\n",
    "   This component downloads the actual images based on the URLs retrieved by the previous component.\n",
    "   It takes in the URLs as input and returns the actual images.\n",
    "\n",
    "3. [**Resize images**](https://github.com/ml6team/fondant/tree/main/components/resize_images): This\n",
    "   component resizes the images to a fixed size. It takes in the images as input and returns the\n",
    "   resized images.\n",
    "\n",
    "4. [**Detect text**](components/detect_text): This component detects text in the images using\n",
    "   ann [mmocr model](https://github.com/locuslab/T-MARS/tree/main/dataset2metadata/text_detection).\n",
    "   It takes in the images as input and returns the bounding boxes of the detected text.\n",
    "\n",
    "5. [**Mask images**](components/mask_images): This component masks the detected text in the images.\n",
    "   It takes in the images and the bounding boxes as input and returns the masked images.\n",
    "\n",
    "6. [**Add clip score**](components/add_clip_score): This component adds a CLIP score to the images.\n",
    "   The clip score is estimated as the dot product between the CLIP embeddings of the masked images\n",
    "   and the original image captions.\n",
    "\n",
    "7. [**Filter clip score**](components/filter_clip_score): This component filters the images based on\n",
    "   their CLIP score. It takes in the images and the CLIP scores as input and returns the filtered\n",
    "   indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisite:**\n",
    "\n",
    "- Ensure Python version 3.8 to 3.10 is installed on your system.\n",
    "- Install and configure Docker on your system.\n",
    "- Ensure that you have a GPU for running the GPU-based component of the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup your environment \n",
    "!pip install \"fondant[docker]==0.8.0\" -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the pipeline\n",
    "\n",
    "First of all, we need to initialize the pipeline, which includes specifying a name for your pipeline, providing a description, and setting a base_path. The base_path is used to store the pipeline artifacts and data generated by the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import fsspec\n",
    "import subprocess\n",
    "\n",
    "from fondant.pipeline import Pipeline, Resources\n",
    "\n",
    "# Check GPU\n",
    "try:\n",
    "    subprocess.check_output('nvidia-smi')\n",
    "    number_of_accelerators = 1\n",
    "    accelerator_name = \"GPU\"\n",
    "except Exception:\n",
    "    logging.warning(\"We recommend to run this pipeline on a GPU, but none could be found\")\n",
    "    number_of_accelerators = None\n",
    "    accelerator_name = None\n",
    "    \n",
    "# General configs\n",
    "BASE_PATH = \"./fondant-artifacts\"\n",
    "N_ROWS_TO_LOAD = 10  # Set to None to load all rows\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "# Create data directory if it doesn't exist and if it's a local path\n",
    "if fsspec.core.url_to_fs(BASE_PATH)[0].protocol == ('file', 'local'):\n",
    "    Path(BASE_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=\"datacomp-filtering-pipeline\",\n",
    "    description=\"A pipeline for filtering the Datacomp dataset\",\n",
    "    base_path=BASE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we will use the `load_from_hub_op` component to load the initial [dataset](https://huggingface.co/datasets/nielsr/datacomp-small-with-text-embeddings):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_from_hf_hub = pipeline.read(\n",
    "    \"load_from_hf_hub\",\n",
    "    arguments={\n",
    "        \"dataset_name\": \"nielsr/datacomp-small-with-text-embeddings\",\n",
    "        \"n_rows_to_load\": N_ROWS_TO_LOAD,\n",
    "    },\n",
    "    produces={\n",
    "        \"url\": pa.string(),\n",
    "        \"original_width\": pa.int64(),\n",
    "        \"original_height\": pa.int64(),\n",
    "        \"face_bboxes\": pa.list_(pa.list_(pa.float64())),\n",
    "        \"sha256\": pa.string(),\n",
    "        \"text\": pa.string(),\n",
    "        \"uid\": pa.string(),\n",
    "        \"clip_b32_similarity_score\": pa.float32(),\n",
    "        \"clip_l14_similarity_score\": pa.float32(),\n",
    "        \"clip_l14_text_embedding\": pa.list_(pa.float64())\n",
    "    },\n",
    "    cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our pipeline consists of a single component that loads the dataset from HuggingFace Hub. We can proceed to add the other components. To add a new reusable component, use the `apply` method. We have to pass the name of the component we want to use, as well as component arguments.\n",
    "\n",
    "The `consumes` argument defines which columns of the dataset will be passed to component. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = dataset_from_hf_hub.apply(\n",
    "    \"download_images\",\n",
    "    consumes={\n",
    "        \"image_url\": \"url\"\n",
    "    },\n",
    "    arguments={\n",
    "        \"retries\": 2,\n",
    "        \"min_image_size\": 0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can utilize the `apply` method to incorporate custom components. For this, it is necessary to provide the path to the implementation of the custom component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resized_images = images.apply(\n",
    "     \"components/resize_images\",\n",
    "     arguments={\n",
    "         \"resize_width\": IMAGE_SIZE,\n",
    "         \"resize_height\": IMAGE_SIZE,\n",
    "     }\n",
    " )\n",
    "\n",
    "detected_text = resized_images.apply(\n",
    "    \"components/detect_text\",\n",
    "    arguments={\n",
    "        \"batch_size\": 8,\n",
    "        \"image_size\": IMAGE_SIZE,\n",
    "    },\n",
    "    resources=Resources(accelerator_name=\"GPU\", accelerator_number=1),\n",
    "    cache=False\n",
    ")\n",
    "\n",
    "mask_images = detected_text.apply(\n",
    "    \"components/mask_images\", \n",
    "    cache=False\n",
    ")\n",
    "\n",
    "embedded_images = mask_images.apply(\n",
    "    \"embed_images\",\n",
    "    arguments={\n",
    "        \"batch_size\": 8,\n",
    "    },\n",
    "    resources=Resources(accelerator_name=\"GPU\", accelerator_number=1)\n",
    ")\n",
    "\n",
    "images_with_clip_score = embedded_images.apply(\n",
    "    \"components/add_clip_score\",\n",
    "    consumes={\n",
    "        \"text_embedding\": \"clip_l14_text_embedding\"\n",
    "    }\n",
    ")\n",
    "\n",
    "filtered_clip_score_op = images_with_clip_score.apply(\n",
    "    \"components/filter_clip_score\",\n",
    "    arguments={\n",
    "        \"threshold_score\": 0.19\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline\n",
    "\n",
    "The pipeline will generate the prompts, retreive matching images in the laion dataset and download then and finally will generate corresponding captions and segmentations needed before writing the dataset to the HF hub.\n",
    "\n",
    "We can execute our pipeline. Fondant provides various executors, and in this case, we are using the `DockerRunner` for local execution, which utilizes docker-compose under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-12-20 09:23:27,598 | root | INFO] Found reference to un-compiled pipeline... compiling\n",
      "[2023-12-20 09:23:27,599 | fondant.pipeline.compiler | INFO] Compiling datacomp-filtering-pipeline to .fondant/compose.yaml\n",
      "[2023-12-20 09:23:27,600 | fondant.pipeline.compiler | INFO] Base path found on local system, setting up ./fondant-artifacts as mount volume\n",
      "[2023-12-20 09:23:27,601 | fondant.pipeline.pipeline | INFO] Sorting pipeline component graph topologically.\n",
      "[2023-12-20 09:23:27,612 | fondant.pipeline.pipeline | INFO] All pipeline component specifications match.\n",
      "[2023-12-20 09:23:27,613 | fondant.pipeline.compiler | INFO] Compiling service for load_from_hugging_face_hub\n",
      "[2023-12-20 09:23:27,614 | fondant.pipeline.compiler | INFO] Compiling service for download_images\n",
      "[2023-12-20 09:23:27,616 | fondant.pipeline.compiler | INFO] Compiling service for resize_images\n",
      "[2023-12-20 09:23:27,617 | fondant.pipeline.compiler | INFO] Found Dockerfile for resize_images, adding build step.\n",
      "[2023-12-20 09:23:27,618 | fondant.pipeline.compiler | INFO] Compiling service for detect_text\n",
      "[2023-12-20 09:23:27,618 | fondant.pipeline.compiler | INFO] Found Dockerfile for detect_text, adding build step.\n",
      "[2023-12-20 09:23:27,619 | fondant.pipeline.compiler | INFO] Compiling service for mask_images\n",
      "[2023-12-20 09:23:27,620 | fondant.pipeline.compiler | INFO] Found Dockerfile for mask_images, adding build step.\n",
      "[2023-12-20 09:23:27,621 | fondant.pipeline.compiler | INFO] Compiling service for embed_images\n",
      "[2023-12-20 09:23:27,622 | fondant.pipeline.compiler | INFO] Compiling service for add_clip_score\n",
      "[2023-12-20 09:23:27,623 | fondant.pipeline.compiler | INFO] Found Dockerfile for add_clip_score, adding build step.\n",
      "[2023-12-20 09:23:27,626 | fondant.pipeline.compiler | INFO] Compiling service for filter_clip_score\n",
      "[2023-12-20 09:23:27,626 | fondant.pipeline.compiler | INFO] Found Dockerfile for filter_clip_score, adding build step.\n",
      "[2023-12-20 09:23:27,651 | fondant.pipeline.compiler | INFO] Successfully compiled to .fondant/compose.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline run...\n",
      "Finished pipeline run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown shorthand flag: 'f' in -f\n",
      "See 'docker --help'.\n",
      "\n",
      "Usage:  docker [OPTIONS] COMMAND\n",
      "\n",
      "A self-sufficient runtime for containers\n",
      "\n",
      "Common Commands:\n",
      "  run         Create and run a new container from an image\n",
      "  exec        Execute a command in a running container\n",
      "  ps          List containers\n",
      "  build       Build an image from a Dockerfile\n",
      "  pull        Download an image from a registry\n",
      "  push        Upload an image to a registry\n",
      "  images      List images\n",
      "  login       Log in to a registry\n",
      "  logout      Log out from a registry\n",
      "  search      Search Docker Hub for images\n",
      "  version     Show the Docker version information\n",
      "  info        Display system-wide information\n",
      "\n",
      "Management Commands:\n",
      "  builder     Manage builds\n",
      "  container   Manage containers\n",
      "  context     Manage contexts\n",
      "  image       Manage images\n",
      "  manifest    Manage Docker image manifests and manifest lists\n",
      "  network     Manage networks\n",
      "  plugin      Manage plugins\n",
      "  scan*       Docker Scan (Docker Inc., v0.23.0)\n",
      "  system      Manage Docker\n",
      "  trust       Manage trust on Docker images\n",
      "  volume      Manage volumes\n",
      "\n",
      "Swarm Commands:\n",
      "  swarm       Manage Swarm\n",
      "\n",
      "Commands:\n",
      "  attach      Attach local standard input, output, and error streams to a running container\n",
      "  commit      Create a new image from a container's changes\n",
      "  cp          Copy files/folders between a container and the local filesystem\n",
      "  create      Create a new container\n",
      "  diff        Inspect changes to files or directories on a container's filesystem\n",
      "  events      Get real time events from the server\n",
      "  export      Export a container's filesystem as a tar archive\n",
      "  history     Show the history of an image\n",
      "  import      Import the contents from a tarball to create a filesystem image\n",
      "  inspect     Return low-level information on Docker objects\n",
      "  kill        Kill one or more running containers\n",
      "  load        Load an image from a tar archive or STDIN\n",
      "  logs        Fetch the logs of a container\n",
      "  pause       Pause all processes within one or more containers\n",
      "  port        List port mappings or a specific mapping for the container\n",
      "  rename      Rename a container\n",
      "  restart     Restart one or more containers\n",
      "  rm          Remove one or more containers\n",
      "  rmi         Remove one or more images\n",
      "  save        Save one or more images to a tar archive (streamed to STDOUT by default)\n",
      "  start       Start one or more stopped containers\n",
      "  stats       Display a live stream of container(s) resource usage statistics\n",
      "  stop        Stop one or more running containers\n",
      "  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE\n",
      "  top         Display the running processes of a container\n",
      "  unpause     Unpause all processes within one or more containers\n",
      "  update      Update configuration of one or more containers\n",
      "  wait        Block until one or more containers stop, then print their exit codes\n",
      "\n",
      "Global Options:\n",
      "      --config string      Location of client config files (default\n",
      "                           \"/home/jupyter/.docker\")\n",
      "  -c, --context string     Name of the context to use to connect to the\n",
      "                           daemon (overrides DOCKER_HOST env var and\n",
      "                           default context set with \"docker context use\")\n",
      "  -D, --debug              Enable debug mode\n",
      "  -H, --host list          Daemon socket to connect to\n",
      "  -l, --log-level string   Set the logging level (\"debug\", \"info\",\n",
      "                           \"warn\", \"error\", \"fatal\") (default \"info\")\n",
      "      --tls                Use TLS; implied by --tlsverify\n",
      "      --tlscacert string   Trust certs signed only by this CA (default\n",
      "                           \"/home/jupyter/.docker/ca.pem\")\n",
      "      --tlscert string     Path to TLS certificate file (default\n",
      "                           \"/home/jupyter/.docker/cert.pem\")\n",
      "      --tlskey string      Path to TLS key file (default\n",
      "                           \"/home/jupyter/.docker/key.pem\")\n",
      "      --tlsverify          Use TLS and verify the remote\n",
      "  -v, --version            Print version information and quit\n",
      "\n",
      "Run 'docker COMMAND --help' for more information on a command.\n",
      "\n",
      "For more help on how to use Docker, head to https://docs.docker.com/go/guides/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fondant.pipeline.runner import DockerRunner\n",
    "DockerRunner().run(input=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explore the dataset using the fondant explorer, this enables you to visualize your output dataset at each component step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "run_explorer_app(base_path=BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up\n",
    "\n",
    "If you're happy with your dataset, it's time to scale up. Check [our documentation](https://fondant.ai/en/latest/) for more information about the available runners."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m114",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m114"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
