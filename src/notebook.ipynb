{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üç´ Building a Datacomp filtering pipeline with Fondant\n",
    "\n",
    "[DataComp](https://www.datacomp.ai/) is a competition organized by the University of Washington and\n",
    "others to come up with the best possible image-text dataset to train a fixed CLIP model. Hence, it's\n",
    "an ideal use case for Fondant, as we can leverage reusable components to filter large, noisy\n",
    "image-text datasets.\n",
    "\n",
    "In this example, we build a pipeline for filtering the dataset using the T-Mars data filtering\n",
    "approach. For more information on T-Mars, check out\n",
    "the [official paper](https://arxiv.org/pdf/2307.03132.pdf). \n",
    "\n",
    "There are 7 components in total, these are:\n",
    "\n",
    "1. [**Load from hf hub**](components/generate_prompts): The pipeline begins by loading the initial\n",
    "   datacomp data which we hosted on the Hugginface hub.\n",
    "\n",
    "2. [**Download images**](https://github.com/ml6team/fondant/tree/main/components/download_images):\n",
    "   This component downloads the actual images based on the URLs retrieved by the previous component.\n",
    "   It takes in the URLs as input and returns the actual images.\n",
    "\n",
    "3. [**Resize images**](https://github.com/ml6team/fondant/tree/main/components/resize_images): This\n",
    "   component resizes the images to a fixed size. It takes in the images as input and returns the\n",
    "   resized images.\n",
    "\n",
    "4. [**Detect text**](components/detect_text): This component detects text in the images using\n",
    "   ann [mmocr model](https://github.com/locuslab/T-MARS/tree/main/dataset2metadata/text_detection).\n",
    "   It takes in the images as input and returns the bounding boxes of the detected text.\n",
    "\n",
    "5. [**Mask images**](components/mask_images): This component masks the detected text in the images.\n",
    "   It takes in the images and the bounding boxes as input and returns the masked images.\n",
    "\n",
    "6. [**Add clip score**](components/add_clip_score): This component adds a CLIP score to the images.\n",
    "   The clip score is estimated as the dot product between the CLIP embeddings of the masked images\n",
    "   and the original image captions.\n",
    "\n",
    "7. [**Filter clip score**](components/filter_clip_score): This component filters the images based on\n",
    "   their CLIP score. It takes in the images and the CLIP scores as input and returns the filtered\n",
    "   indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisite:**\n",
    "\n",
    "- Ensure Python version 3.8 to 3.10 is installed on your system.\n",
    "- Install and configure Docker on your system.\n",
    "- Ensure that you have a GPU for running the GPU-based component of the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "### This section checks the prerequisites of your environment. Read any errors or warnings carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensure a Python version between 3.8 and 3.10 is available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info < (3, 8, 0) or sys.version_info >= (3, 11, 0):\n",
    "    raise Exception(f\"A Python version between 3.8 and 3.10 is required. You are running {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if docker compose is installed and the docker daemon is running**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose version >/dev/null\n",
    "!docker info >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if GPU is available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    subprocess.check_output('nvidia-smi')\n",
    "    logging.info(\"Found GPU, using it!\")\n",
    "    number_of_accelerators = 1\n",
    "    accelerator_name = \"GPU\"\n",
    "except Exception:\n",
    "    logging.warning(\"We recommend to run this pipeline on a GPU, but none could be found, using CPU instead\")\n",
    "    number_of_accelerators = None\n",
    "    accelerator_name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Fondant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the pipeline\n",
    "\n",
    "First of all, we need to initialize the pipeline, which includes specifying a name for your pipeline, providing a description, and setting a base_path. The base_path is used to store the pipeline artifacts and data generated by the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from fondant.pipeline import ComponentOp, Pipeline, Resources\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "BASE_PATH = \"./data_dir\"\n",
    "Path(BASE_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"controlnet-pipeline\",\n",
    "    pipeline_description=\"Pipeline that collects data to train ControlNet\",\n",
    "    base_path=BASE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we will use the `load_from_hub_op` component to load the initial [dataset](https://huggingface.co/datasets/nielsr/datacomp-small-with-text-embeddings):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_component_column_mapping = {\n",
    "    \"url\": \"images_url\",\n",
    "    \"original_width\": \"images_width\",\n",
    "    \"original_height\": \"images_height\",\n",
    "    \"face_bboxes\": \"images_face_bboxes\",\n",
    "    \"sha256\": \"images_sha256\",\n",
    "    \"text\": \"text_data\",\n",
    "    \"uid\": \"image_text_uid\",\n",
    "    \"clip_b32_similarity_score\": \"image_text_clip_b32_similarity_score\",\n",
    "    \"clip_l14_similarity_score\": \"image_text_clip_l14_similarity_score\",\n",
    "}\n",
    "\n",
    "\n",
    "load_from_hub_op = ComponentOp(\n",
    "    component_dir=\"components/load_from_hf_hub\",\n",
    "    arguments={\n",
    "        \"dataset_name\": \"nielsr/datacomp-small-with-text-embeddings\",\n",
    "        \"column_name_mapping\": load_component_column_mapping,\n",
    "        \"n_rows_to_load\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "pipeline.add_op(load_from_hub_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our pipeline consists of a single component that loads the dataset from HuggingFace Hub. We can proceed to add the other components. The resuable components available on the hub will be loaded using the `ComponentOp.from_registry(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_images_op = ComponentOp.from_registry(\n",
    "    name=\"download_images\",\n",
    "    arguments={\n",
    "        \"retries\": 2,\n",
    "        \"min_image_size\": 0,\n",
    "    },\n",
    ")\n",
    "\n",
    "resize_images = ComponentOp(\n",
    "    component_dir=\"components/resize_images\",\n",
    "    arguments={\n",
    "        \"resize_width\": IMAGE_SIZE,\n",
    "        \"resize_height\": IMAGE_SIZE,\n",
    "    },\n",
    ")\n",
    "\n",
    "detect_text_op = ComponentOp(\n",
    "    component_dir=\"components/detect_text\",\n",
    "    arguments={\n",
    "        \"batch_size\": 8,\n",
    "        \"image_size\": IMAGE_SIZE,\n",
    "    },\n",
    "    resources=Resources(\n",
    "        accelerator_number=number_of_accelerators,\n",
    "        accelerator_name=accelerator_name,\n",
    "    ),\n",
    ")\n",
    "mask_images_op = ComponentOp(\n",
    "    component_dir=\"components/mask_images\",\n",
    ")\n",
    "\n",
    "embed_images_op = ComponentOp.from_registry(\n",
    "    name=\"embed_images\",\n",
    "    arguments={\n",
    "        \"batch_size\": 8,\n",
    "    },\n",
    "    resources=Resources(\n",
    "        accelerator_number=number_of_accelerators,\n",
    "        accelerator_name=accelerator_name,\n",
    "    ),\n",
    ")\n",
    "add_clip_score_op = ComponentOp(\n",
    "    component_dir=\"components/add_clip_score\",\n",
    ")\n",
    "\n",
    "filter_clip_score_op = ComponentOp(\n",
    "    component_dir=\"components/filter_clip_score\",\n",
    "    arguments={\n",
    "        \"threshold_score\": 0.19,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the components in our pipeline. It is important to note that we will define dependencies between the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_op(download_images_op, dependencies=load_from_hub_op)\n",
    "pipeline.add_op(resize_images, dependencies=download_images_op)\n",
    "pipeline.add_op(detect_text_op, dependencies=resize_images)\n",
    "pipeline.add_op(mask_images_op, dependencies=detect_text_op)\n",
    "pipeline.add_op(embed_images_op, dependencies=mask_images_op)\n",
    "pipeline.add_op(add_clip_score_op, dependencies=embed_images_op)\n",
    "pipeline.add_op(filter_clip_score_op, dependencies=add_clip_score_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline\n",
    "\n",
    "The pipeline will generate the prompts, retreive matching images in the laion dataset and download then and finally will generate corresponding captions and segmentations needed before writing the dataset to the HF hub.\n",
    "\n",
    "We can execute our pipeline. Fondant provides various executors, and in this case, we are using the `DockerRunner` for local execution, which utilizes docker-compose under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.pipeline.compiler import DockerCompiler\n",
    "from fondant.pipeline.runner import DockerRunner\n",
    "\n",
    "DockerCompiler().compile(pipeline=pipeline, output_path = \"docker-compose.yml\")\n",
    "DockerRunner().run(\"docker-compose.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explore the dataset using the fondant explorer, this enables you to visualize your output dataset at each component step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "\n",
    "run_explorer_app(\n",
    "    base_path=BASE_PATH,\n",
    "    container=\"fndnt/data_explorer\",\n",
    "    tag=\"latest\",\n",
    "    port=8501,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
