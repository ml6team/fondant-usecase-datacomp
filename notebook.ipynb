{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ« Building a Datacomp filtering pipeline with Fondant\n",
    "\n",
    "[DataComp](https://www.datacomp.ai/) is a competition organized by the University of Washington and\n",
    "others to come up with the best possible image-text dataset to train a fixed CLIP model. Hence, it's\n",
    "an ideal use case for Fondant, as we can leverage reusable components to filter large, noisy\n",
    "image-text datasets.\n",
    "\n",
    "In this example, we build a pipeline for filtering the dataset using the T-Mars data filtering\n",
    "approach. For more information on T-Mars, check out\n",
    "the [official paper](https://arxiv.org/pdf/2307.03132.pdf). \n",
    "\n",
    "There are 7 components in total, these are:\n",
    "\n",
    "1. [**Load from hf hub**](components/generate_prompts): The pipeline begins by loading the initial\n",
    "   datacomp data which we hosted on the Hugginface hub.\n",
    "\n",
    "2. [**Download images**](https://github.com/ml6team/fondant/tree/main/components/download_images):\n",
    "   This component downloads the actual images based on the URLs retrieved by the previous component.\n",
    "   It takes in the URLs as input and returns the actual images.\n",
    "\n",
    "3. [**Resize images**](https://github.com/ml6team/fondant/tree/main/components/resize_images): This\n",
    "   component resizes the images to a fixed size. It takes in the images as input and returns the\n",
    "   resized images.\n",
    "\n",
    "4. [**Detect text**](components/detect_text): This component detects text in the images using\n",
    "   ann [mmocr model](https://github.com/locuslab/T-MARS/tree/main/dataset2metadata/text_detection).\n",
    "   It takes in the images as input and returns the bounding boxes of the detected text.\n",
    "\n",
    "5. [**Mask images**](components/mask_images): This component masks the detected text in the images.\n",
    "   It takes in the images and the bounding boxes as input and returns the masked images.\n",
    "\n",
    "6. [**Add clip score**](components/add_clip_score): This component adds a CLIP score to the images.\n",
    "   The clip score is estimated as the dot product between the CLIP embeddings of the masked images\n",
    "   and the original image captions.\n",
    "\n",
    "7. [**Filter clip score**](components/filter_clip_score): This component filters the images based on\n",
    "   their CLIP score. It takes in the images and the CLIP scores as input and returns the filtered\n",
    "   indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisite:**\n",
    "\n",
    "- Ensure Python version 3.8 to 3.10 is installed on your system.\n",
    "- Install and configure Docker on your system.\n",
    "- Ensure that you have a GPU for running the GPU-based component of the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Setup your environment \n",
    "!pip install \"fondant[docker]==0.6.2\" -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the pipeline\n",
    "\n",
    "First of all, we need to initialize the pipeline, which includes specifying a name for your pipeline, providing a description, and setting a base_path. The base_path is used to store the pipeline artifacts and data generated by the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "import fsspec\n",
    "\n",
    "from fondant.pipeline import ComponentOp, Pipeline\n",
    "from fondant.compiler import DockerCompiler\n",
    "from fondant.runner import DockerRunner\n",
    "\n",
    "# General configs\n",
    "BASE_PATH = \"./data_dir\"\n",
    "N_ROWS_TO_LOAD = 10  # Set to None to load all rows\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "# Create data directory if it doesn't exist and if it's a local path\n",
    "if fsspec.core.url_to_fs(BASE_PATH)[0].protocol == ('file', 'local'):\n",
    "    Path(BASE_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"datacomp-filtering-pipeline\",\n",
    "    pipeline_description=\"A pipeline for filtering the Datacomp dataset\",\n",
    "    base_path=BASE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we will use the `load_from_hub_op` component to load the initial [dataset](https://huggingface.co/datasets/nielsr/datacomp-small-with-text-embeddings):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_component_column_mapping = {\n",
    "    \"url\": \"images_url\",\n",
    "    \"original_width\": \"images_width\",\n",
    "    \"original_height\": \"images_height\",\n",
    "    \"face_bboxes\": \"images_face_bboxes\",\n",
    "    \"sha256\": \"images_sha256\",\n",
    "    \"text\": \"text_data\",\n",
    "    \"uid\": \"image_text_uid\",\n",
    "    \"clip_b32_similarity_score\": \"image_text_clip_b32_similarity_score\",\n",
    "    \"clip_l14_similarity_score\": \"image_text_clip_l14_similarity_score\",\n",
    "}\n",
    "\n",
    "\n",
    "load_from_hub_op = ComponentOp(\n",
    "    component_dir=\"components/load_from_hf_hub\",\n",
    "    arguments={\n",
    "        \"dataset_name\": \"nielsr/datacomp-small-with-text-embeddings\",\n",
    "        \"column_name_mapping\": load_component_column_mapping,\n",
    "        \"n_rows_to_load\": N_ROWS_TO_LOAD,\n",
    "    },\n",
    ")\n",
    "\n",
    "pipeline.add_op(load_from_hub_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our pipeline consists of a single component that loads the dataset from HuggingFace Hub. We can proceed to add the other components. The resuable components available on the hub will be loaded using the `ComponentOp.from_registry(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_images_op = ComponentOp.from_registry(\n",
    "    name=\"download_images\",\n",
    "    arguments={\n",
    "        \"retries\": 2,\n",
    "        \"min_image_size\": 0,\n",
    "    },\n",
    ")\n",
    "\n",
    "resize_images = ComponentOp(\n",
    "    component_dir=\"components/resize_images\",\n",
    "    arguments={\n",
    "        \"resize_width\": IMAGE_SIZE,\n",
    "        \"resize_height\": IMAGE_SIZE,\n",
    "    },\n",
    ")\n",
    "\n",
    "detect_text_op = ComponentOp(\n",
    "    component_dir=\"components/detect_text\",\n",
    "    arguments={\n",
    "        \"batch_size\": 8,\n",
    "        \"image_size\": IMAGE_SIZE,\n",
    "    },\n",
    "    accelerator_name=\"GPU\",\n",
    "    number_of_accelerators=1,\n",
    ")\n",
    "mask_images_op = ComponentOp(\n",
    "    component_dir=\"components/mask_images\",\n",
    ")\n",
    "\n",
    "embed_images_op = ComponentOp.from_registry(\n",
    "    name=\"embed_images\",\n",
    "    arguments={\n",
    "        \"batch_size\": 8,\n",
    "    },\n",
    "    accelerator_name=\"GPU\",\n",
    "    number_of_accelerators=1,\n",
    ")\n",
    "add_clip_score_op = ComponentOp(\n",
    "    component_dir=\"components/add_clip_score\",\n",
    ")\n",
    "\n",
    "filter_clip_score_op = ComponentOp(\n",
    "    component_dir=\"components/filter_clip_score\",\n",
    "    arguments={\n",
    "        \"threshold_score\": 0.19,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the components in our pipeline. It is important to note that we will define dependencies between the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_op(laion_retrieval_op, dependencies=generate_prompts_op)\n",
    "pipeline.add_op(download_images_op, dependencies=laion_retrieval_op)\n",
    "pipeline.add_op(caption_images_op, dependencies=download_images_op)\n",
    "pipeline.add_op(segment_images_op, dependencies=caption_images_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: writing the dataset to HF hub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the final dataset to HF hub, we will use the `write_to_hf_hub` component. This component is a reusable Fondant component which is **generic**. This implies that we still need to customize the component specification file. We have to modify the dataframe schema defined in the `consumes` section of the component.\n",
    "\n",
    "To achieve this, we can create a `fondant_component.yaml` file in the directory `components/write_to_hf_hub` with the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/write_to_hub_controlnet/fondant_component.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/write_to_hub_controlnet/fondant_component.yaml\n",
    "name: Write to hub\n",
    "description: Component that writes a dataset to the hub\n",
    "image: fndnt/write_to_hf_hub:0.6.2\n",
    "\n",
    "consumes:\n",
    "  images:\n",
    "    fields:\n",
    "      data:\n",
    "        type: binary\n",
    "\n",
    "  captions:\n",
    "    fields:\n",
    "      text:\n",
    "        type: string\n",
    "\n",
    "  segmentations:\n",
    "    fields:\n",
    "      data:\n",
    "        type: binary\n",
    "\n",
    "args:\n",
    "  hf_token:\n",
    "    description: The hugging face token used to write to the hub\n",
    "    type: str\n",
    "  username:\n",
    "    description: The username under which to upload the dataset\n",
    "    type: str\n",
    "  dataset_name:\n",
    "    description: The name of the dataset to upload\n",
    "    type: str\n",
    "  image_column_names:\n",
    "    description: A list containing the image column names. Used to format to image to HF hub format\n",
    "    type: list\n",
    "    default: []\n",
    "  column_name_mapping:\n",
    "    description: Mapping of the consumed fondant column names to the written hub column names\n",
    "    type: dict\n",
    "    default: {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = \n",
    "HF_TOKEN = \n",
    "\n",
    "write_to_hub_controlnet = ComponentOp(\n",
    "    component_dir=\"components/write_to_hub_controlnet\",\n",
    "    arguments={\n",
    "        \"username\": USERNAME ,\n",
    "        \"hf_token\": HF_TOKEN ,\n",
    "        \"dataset_name\": \"controlnet-interior-design\",\n",
    "        \"image_column_names\": [\"images_data\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_op(write_to_hub_controlnet, dependencies=segment_images_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline\n",
    "\n",
    "The pipeline will generate the prompts, retreive matching images in the laion dataset and download then and finally will generate corresponding captions and segmentations needed before writing the dataset to the HF hub.\n",
    "\n",
    "We can execute our pipeline. Fondant provides various executors, and in this case, we are using the `DockerRunner` for local execution, which utilizes docker-compose under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler = DockerCompiler()\n",
    "runner = DockerRunner()\n",
    "\n",
    "compiler.compile(pipeline=pipeline, output_path = \"docker-compose.yml\")\n",
    "DockerRunner().run(\"docker-compose.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explore the dataset using the fondant explorer, this enables you to visualize your output dataset at each component step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "\n",
    "run_explorer_app(\n",
    "    base_path=BASE_PATH,\n",
    "    container=\"fndnt/data_explorer\",\n",
    "    tag=\"latest\",\n",
    "    port=8501,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
